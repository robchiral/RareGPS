{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def cb(series):\n",
    "    return series.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "def bootstrap_ci(y_true, y_pred, n_bootstraps=2000, alpha=0.05):\n",
    "    bootstrapped_scores = {'auroc': [], 'auprc': []}\n",
    "    indices = np.arange(len(y_true))\n",
    "    for _ in range(n_bootstraps):\n",
    "        sampled_indices = resample(indices, replace=True)\n",
    "        bootstrapped_scores['auroc'].append(roc_auc_score(y_true[sampled_indices], y_pred[sampled_indices]))\n",
    "        bootstrapped_scores['auprc'].append(average_precision_score(y_true[sampled_indices], y_pred[sampled_indices]))\n",
    "\n",
    "    ci = {}\n",
    "    for score_type in bootstrapped_scores:\n",
    "        sorted_scores = np.sort(bootstrapped_scores[score_type])\n",
    "        lower = np.percentile(sorted_scores, 100 * alpha / 2)\n",
    "        upper = np.percentile(sorted_scores, 100 * (1 - alpha / 2))\n",
    "        mean = np.mean(sorted_scores)\n",
    "        ci[score_type] = (lower, mean, upper)\n",
    "    return ci\n",
    "\n",
    "cov = ['loeuf'] \n",
    "ot = ['clin_ot','hgmd','ot_genetics_portal','expression_atlas','impc','europepmc']\n",
    "mantis = ['mantis']\n",
    "cc = ['cc_common_max_p','cc_rare_max_p','cc_rare_burden_max_p','cc_ultrarare_max_p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files referenced (including \"ml_input.pkl\") are not currently available but will be released upon publication. This file will contain cleaned data for all phecodes in phecodeX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the dataframe containing input data and select phecodes of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_pickle('./Examples/ml_input.pkl')\n",
    "\n",
    "if False:\n",
    "    phecode_list = [] # Selected phecodes\n",
    "    alldata = alldata.loc[alldata['phecode'].isin(phecode_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train RareGPS using a logistic regression (\"reg:logistic\") XGBoost model where we assign true values probabilities from 0 to 1 based on the maximum clinical trial phase for the G-P pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata['xscore'] = 0.0\n",
    "alldata.loc[alldata['phase'] == 0.5, 'xscore'] = 1*0.732*0.548*0.580*0.911\n",
    "alldata.loc[alldata['phase'] == 1, 'xscore'] = 1*0.732*0.548*0.580\n",
    "alldata.loc[alldata['phase'] == 2, 'xscore'] = 1*0.732*0.548\n",
    "alldata.loc[alldata['phase'] == 3, 'xscore'] = 1*0.732\n",
    "alldata.loc[alldata['phase'] == 4, 'xscore'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend training RareGPS only on G-P pairs representing druggable genes ('drug_gene' == 1) and phecodes with at least one drug indication ('indication' == 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indata = alldata.loc[alldata['drug_gene'] == 1]\n",
    "ind = indata.groupby('phecode')['indication'].max().reset_index()\n",
    "ind = ind.loc[ind['indication'] == 1]\n",
    "indata = indata.loc[indata['phecode'].isin(ind['phecode'])]\n",
    "print(indata['phecode'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate separate training and external testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = indata.loc[indata['source'].astype(str) != \"['Minikel']\"]\n",
    "test_set = indata.loc[(indata['source'].astype(str).str.contains('Minikel')) | (indata['indication'] == 0)]\n",
    "\n",
    "ind = train_set.groupby('phecode')['indication'].max().reset_index()\n",
    "ind = ind.loc[ind['indication'] == 1]\n",
    "train_set = train_set.loc[train_set['phecode'].isin(ind['phecode'])]\n",
    "print(train_set['phecode'].nunique())\n",
    "\n",
    "ind = test_set.groupby('phecode')['indication'].max().reset_index()\n",
    "ind = ind.loc[ind['indication'] == 1]\n",
    "test_set = test_set.loc[test_set['phecode'].isin(ind['phecode'])]\n",
    "print(test_set['phecode'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wish to generate predictions for all protein coding genes where at least one of the features is nonzero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc = alldata.loc[alldata[ot+mantis+cc].max(axis=1) > 0][['id','gene','phecode','indication','phase','xscore']+cov+ot+mantis+cc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform hyperparameter tuning and examine AUROC and AUPRC for holdout set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test = {\n",
    "    'objective': ['reg:logistic'],\n",
    "    'booster': ['gbtree'],\n",
    "    'max_depth': [5,6,7],\n",
    "    'alpha': [1,2,5,7,10,20],\n",
    "    'lambda': [1,2,5,7,10],\n",
    "    'min_child_weight': [1,2,5,7,10,20],\n",
    "    'subsample': [1],\n",
    "    'colsample_bytree': [1],\n",
    "    'nthread': [16]\n",
    "}\n",
    "keys, values = zip(*params_test.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "df = pd.DataFrame(combinations)\n",
    "param_list = df.sample(n=40).to_dict(orient='records')\n",
    "pi = 0\n",
    "\n",
    "mnum = 0 # Select model\n",
    "model_names = [['RareGPS_No_GA','RareGPS'][mnum]]\n",
    "feature_names = [[cov+ot+mantis,cov+ot+mantis+cc][mnum]]\n",
    "\n",
    "for i,j in zip(model_names, feature_names):\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for params in param_list:\n",
    "        print(pi)\n",
    "        pi+=1\n",
    "        \n",
    "        X = train_set[j]\n",
    "        y = train_set['xscore']\n",
    "        ids = train_set['id']\n",
    "        \n",
    "        outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        holdout_predictions = []\n",
    "        \n",
    "        for fold, (train_val_idx, holdout_idx) in enumerate(outer_cv.split(X), 1):\n",
    "            print(f\"Fold {fold}\")\n",
    "            \n",
    "            X_train_val, X_holdout = X.iloc[train_val_idx], X.iloc[holdout_idx]\n",
    "            y_train_val, y_holdout = y.iloc[train_val_idx], y.iloc[holdout_idx]\n",
    "            ids_train_val, ids_holdout = ids.iloc[train_val_idx], ids.iloc[holdout_idx]\n",
    "            \n",
    "            inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            \n",
    "            for inner_fold, (train_idx, val_idx) in enumerate(inner_cv.split(X_train_val), 1):\n",
    "                print(f\"  Inner Fold {inner_fold}\")\n",
    "                \n",
    "                X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "                y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "                ids_train, ids_val = ids_train_val.iloc[train_idx], ids_train_val.iloc[val_idx]\n",
    "                \n",
    "                train_data = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "                val_data = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
    "                \n",
    "                evals = [(train_data, 'train'), (val_data, 'eval')]\n",
    "                model = xgb.train(params, train_data, num_boost_round=3000, early_stopping_rounds=10, evals=evals, verbose_eval=False)\n",
    "             \n",
    "                y_pred_hold = model.predict(xgb.DMatrix(X_holdout, enable_categorical=True))\n",
    "                auroc_hold = roc_auc_score(cb(y_holdout), y_pred_hold)\n",
    "                auprc_hold = average_precision_score(cb(y_holdout), y_pred_hold)\n",
    "                print(auroc_hold, auprc_hold)\n",
    "        \n",
    "                fold_predictions = pd.DataFrame({\n",
    "                    'id': ids_holdout,\n",
    "                    'prediction': y_pred_hold,\n",
    "                    'outer_fold': fold,\n",
    "                    'inner_fold': inner_fold\n",
    "                })\n",
    "                holdout_predictions.append(fold_predictions)\n",
    "    \n",
    "                filtered_params = {k: params[k] for k in ['max_depth', 'alpha', 'lambda', 'min_child_weight', 'subsample','colsample_bytree']}\n",
    "                metrics_dict = {'Outer fold': fold, 'Inner fold': inner_fold,\n",
    "                                'AUROC_hold': auroc_hold, 'AUPRC_hold': auprc_hold}\n",
    "                metrics_dict = {**filtered_params, **metrics_dict}\n",
    "                metrics.append(metrics_dict)\n",
    "                print(metrics_dict)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_pickle(f'./Outputs/Tuning/holdout_tuning_{i}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RareGPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using optimal hyperparameters we can then train RareGPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnum = 0 # Select model\n",
    "model_names = [['RareGPS_No_GA','RareGPS'][mnum]]\n",
    "feature_names = [[cov+ot+mantis,cov+ot+mantis+cc][mnum]]\n",
    "\n",
    "for i,j in zip(model_names, feature_names):\n",
    "    print(i)\n",
    "    \n",
    "    X = train_set[j]\n",
    "    y = train_set['xscore']\n",
    "    ids = train_set['id']\n",
    "\n",
    "    holdout_predictions = []    \n",
    "    ext_predictions = []\n",
    "    apc_predictions = pd.DataFrame()\n",
    "    \n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_val_idx, holdout_idx) in enumerate(outer_cv.split(X), 1):\n",
    "        print(f\"Fold {fold}\")\n",
    "        \n",
    "        X_train_val, X_holdout = X.iloc[train_val_idx], X.iloc[holdout_idx]\n",
    "        y_train_val, y_holdout = y.iloc[train_val_idx], y.iloc[holdout_idx]\n",
    "        ids_train_val, ids_holdout = ids.iloc[train_val_idx], ids.iloc[holdout_idx]\n",
    "\n",
    "        apc_predictions_inner = []\n",
    "        \n",
    "        inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for inner_fold, (train_idx, val_idx) in enumerate(inner_cv.split(X_train_val), 1):\n",
    "            print(f\"  Inner Fold {inner_fold}\")\n",
    "            \n",
    "            X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "            y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "            ids_train, ids_val = ids_train_val.iloc[train_idx], ids_train_val.iloc[val_idx]\n",
    "            \n",
    "            ext = test_set.loc[~((test_set['id'].isin(ids_train)) | (test_set['id'].isin(ids_val)))]\n",
    "            X_ext = ext[j]\n",
    "            y_ext = ext['xscore']\n",
    "            ids_ext = ext['id']\n",
    "            \n",
    "            train_data = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "            val_data = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
    "\n",
    "            # Adjust based on hyperparameter tuning\n",
    "            params = {'objective': 'reg:logistic','booster': 'gbtree','max_depth': 7,'alpha': 5,'lambda': 7,'min_child_weight': 1,'nthread': 16}\n",
    "\n",
    "            evals = [(train_data, 'train'), (val_data, 'eval')]\n",
    "            model = xgb.train(params, train_data, num_boost_round=3000, early_stopping_rounds=15, evals=evals, verbose_eval=False)\n",
    "            model.save_model(f\"./Outputs/New models/{i}_{fold}_{inner_fold}.json\")\n",
    "            \n",
    "            # Generate predictions for holdout set\n",
    "            y_pred_hold = model.predict(xgb.DMatrix(X_holdout, enable_categorical=True))\n",
    "            auroc_hold = roc_auc_score(cb(y_holdout), y_pred_hold)\n",
    "            auprc_hold = average_precision_score(cb(y_holdout), y_pred_hold)\n",
    "            print('Holdout', auroc_hold, auprc_hold)\n",
    "\n",
    "            fold_predictions = pd.DataFrame({\n",
    "                'id': ids_holdout,\n",
    "                'prediction': y_pred_hold\n",
    "            })\n",
    "            holdout_predictions.append(fold_predictions)\n",
    "\n",
    "            # Generate predictions for external test set\n",
    "            y_pred_ext = model.predict(xgb.DMatrix(X_ext, enable_categorical=True))\n",
    "            auroc_ext = roc_auc_score(cb(y_ext), y_pred_ext)\n",
    "            auprc_ext = average_precision_score(cb(y_ext), y_pred_ext)\n",
    "            print('External', auroc_ext, auprc_ext)\n",
    "\n",
    "            fold_predictions = pd.DataFrame({\n",
    "                'id': ids_ext,\n",
    "                'prediction': y_pred_ext\n",
    "            })\n",
    "            ext_predictions.append(fold_predictions)\n",
    "\n",
    "            # Generate predictions for all protein coding genes\n",
    "            apc_subset = apc.loc[apc['id'].isin(ids_train_val)]\n",
    "            y_pred_apc = model.predict(xgb.DMatrix(apc_subset[j], enable_categorical=True))\n",
    "            \n",
    "            fold_predictions = pd.DataFrame({\n",
    "                'id': apc_subset['id'],\n",
    "                'prediction': y_pred_apc\n",
    "            })\n",
    "            apc_predictions_inner.append(fold_predictions)            \n",
    "\n",
    "        apc_predictions_inner = pd.concat(apc_predictions_inner, ignore_index=True)\n",
    "        apc_predictions_inner = apc_predictions_inner.groupby('id')['prediction'].mean().reset_index()\n",
    "        apc_predictions = pd.concat([apc_predictions,apc_predictions_inner])\n",
    "\n",
    "    holdout_predictions = pd.concat(holdout_predictions, ignore_index=True)\n",
    "    holdout_predictions = holdout_predictions.groupby('id')['prediction'].mean().reset_index()\n",
    "    all_predictions = holdout_predictions.merge(indata[['id','gene','phecode','indication','phase','xscore']])\n",
    "    all_predictions.to_pickle(f'./Outputs/Predictions/holdout_predictions_{i}.pkl')\n",
    "\n",
    "    ext_predictions = pd.concat(ext_predictions, ignore_index=True)\n",
    "    ext_predictions = ext_predictions.groupby('id')['prediction'].mean().reset_index()\n",
    "    all_predictions = ext_predictions.merge(indata[['id','gene','phecode','indication','phase','xscore']])\n",
    "    all_predictions.to_pickle(f'./Outputs/Predictions/ext_predictions_{i}.pkl')\n",
    "\n",
    "    all_predictions = pd.concat([holdout_predictions,ext_predictions]).drop_duplicates(['id']).reset_index(drop=True)\n",
    "    all_predictions = all_predictions.merge(indata[['id','gene','phecode','indication','phase','xscore']])\n",
    "    all_predictions.to_pickle(f'./Outputs/Predictions/all_predictions_{i}.pkl')\n",
    "\n",
    "    apc_predictions =  apc_predictions.merge(apc[['id','gene','phecode','indication','phase','xscore']])\n",
    "    apc_predictions.to_pickle(f'./Outputs/Predictions/apc_predictions_{i}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds = ['RareGPS_No_GA','RareGPS']\n",
    "\n",
    "results = []\n",
    "for cond in conds:\n",
    "    print('Holdout',cond)\n",
    "    pdf = pd.read_pickle(f'./Outputs/Predictions/holdout_predictions_{cond}.pkl').reset_index(drop=True)\n",
    "    auroc = roc_auc_score(pdf['indication'], pdf['prediction'])\n",
    "    auprc = average_precision_score(pdf['indication'], pdf['prediction'])\n",
    "    ci = bootstrap_ci(pdf['indication'], pdf['prediction'])\n",
    "    results.append({'Dataset':'Holdout','Features':cond,\n",
    "                    'AUROC':auroc, 'AUROC_CI': ci['auroc'],\n",
    "                    'AUPRC':auprc, 'AUPRC_CI': ci['auprc']})\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_excel('./Results/bootstrap_metrics_holdout.xlsx', index=False)\n",
    "\n",
    "results = []\n",
    "for cond in conds:\n",
    "    print('Ext',cond)\n",
    "    pdf = pd.read_pickle(f'./Outputs/Predictions/ext_predictions_{cond}.pkl').reset_index(drop=True)\n",
    "    auroc = roc_auc_score(pdf['indication'], pdf['prediction'])\n",
    "    auprc = average_precision_score(pdf['indication'], pdf['prediction'])\n",
    "    ci = bootstrap_ci(pdf['indication'], pdf['prediction'])\n",
    "    results.append({'Dataset':'External','Features':cond,\n",
    "                    'AUROC':auroc, 'AUROC_CI': ci['auroc'],\n",
    "                    'AUPRC':auprc, 'AUPRC_CI': ci['auprc']})\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_excel('./Results/bootstrap_metrics_ext.xlsx', index=False)\n",
    "\n",
    "results = []\n",
    "for cond in conds:\n",
    "    print('All',cond)\n",
    "    pdf = pd.read_pickle(f'./Outputs/Predictions/all_predictions_{cond}.pkl').reset_index(drop=True)\n",
    "    auroc = roc_auc_score(pdf['indication'], pdf['prediction'])\n",
    "    auprc = average_precision_score(pdf['indication'], pdf['prediction'])\n",
    "    ci = bootstrap_ci(pdf['indication'], pdf['prediction'])\n",
    "    results.append({'Dataset':'Combined','Features':cond,\n",
    "                    'AUROC':auroc, 'AUROC_CI': ci['auroc'],\n",
    "                    'AUPRC':auprc, 'AUPRC_CI': ci['auprc']})\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_excel('./Results/bootstrap_metrics_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_excel('./Results/bootstrap_metrics_holdout.xlsx')\n",
    "b = pd.read_excel('./Results/bootstrap_metrics_ext.xlsx')\n",
    "c = pd.read_excel('./Results/bootstrap_metrics_all.xlsx')\n",
    "metrics = pd.concat([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metrics.copy()\n",
    "df['Dataset'] = pd.Categorical(df['Dataset'], ['Holdout','External','Combined'])\n",
    "df = df.sort_values(['Dataset','Model'])\n",
    "\n",
    "df['AUROC_mean'] = df['AUROC_CI'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: x[1]).astype(float)\n",
    "df['AUROC_CI_lower'] = df['AUROC_CI'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: x[0]).astype(float)\n",
    "df['AUROC_CI_upper'] = df['AUROC_CI'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: x[2]).astype(float)\n",
    "df['error_lower'] = df['AUROC_mean'] - df['AUROC_CI_lower']\n",
    "df['error_upper'] = df['AUROC_CI_upper'] - df['AUROC_mean']\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "sns.set_style('white')\n",
    "\n",
    "datasets = df['Dataset'].unique()\n",
    "models = df['Model'].unique()\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.175 \n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_data = df[df['Model'] == model]\n",
    "    plt.errorbar(x=x + (i - (len(models) - 1) / 2) * width, \n",
    "                 y=model_data['AUROC_mean'], \n",
    "                 yerr=[model_data['error_lower'], model_data['error_upper']], \n",
    "                 fmt='o', capsize=0, label=model, color=CB_color_cycle[i])\n",
    "\n",
    "plt.xticks(ticks=x, labels=datasets)\n",
    "plt.ylabel('AUROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prop = pd.read_pickle(f'./ML/Predictions/holdout_predictions_cov.pkl')['indication'].mean()\n",
    "ext_prop = pd.read_pickle(f'./ML/Predictions/ext_predictions_cov.pkl')['indication'].mean()\n",
    "all_prop = pd.read_pickle(f'./ML/Predictions/all_predictions_cov.pkl')['indication'].mean()\n",
    "\n",
    "df = metrics.loc[~metrics['Features'].isin(['cov_otnopmc','cov_otnopmc_cc'])]\n",
    "df['Proportion'] = df['Dataset'].map({'Holdout':hold_prop,'External':ext_prop,'Combined':all_prop})\n",
    "df['Dataset'] = pd.Categorical(df['Dataset'], ['Holdout','External','Combined'])\n",
    "df = df.sort_values(['Dataset','Model'])\n",
    "\n",
    "df['AUPRC_mean'] = df['AUPRC_CI'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: x[1]).astype(float)\n",
    "df['AUPRC_CI_lower'] = df['AUPRC_CI'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: x[0]).astype(float)\n",
    "df['AUPRC_CI_upper'] = df['AUPRC_CI'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: x[2]).astype(float)\n",
    "df['error_lower'] = df['AUPRC_mean'] - df['AUPRC_CI_lower']\n",
    "df['error_upper'] = df['AUPRC_CI_upper'] - df['AUPRC_mean']\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "sns.set_style('white')\n",
    "\n",
    "datasets = df['Dataset'].unique()\n",
    "models = df['Model'].unique()\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.175 \n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_data = df[df['Model'] == model]\n",
    "    plt.errorbar(x=x + (i - (len(models) - 1) / 2) * width, \n",
    "                 y=model_data['AUPRC_mean'], \n",
    "                 yerr=[model_data['error_lower'], model_data['error_upper']], \n",
    "                 fmt='o', capsize=0, label=model, color=CB_color_cycle[i])\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    proportion = df[df['Dataset'] == dataset]['Proportion'].iloc[0]\n",
    "    left = (x[i] - (len(models) - 1) / 2 * width)\n",
    "    right = (x[i] + (len(models) - 1) / 2 * width)\n",
    "    plt.hlines(y=proportion, xmin=left, xmax=right, color='gray', linestyle='-', linewidth=1)\n",
    "\n",
    "\n",
    "plt.xticks(ticks=x, labels=datasets)\n",
    "plt.ylabel('AUPRC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
