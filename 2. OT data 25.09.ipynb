{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bf5ef2-4f12-4d3f-b85d-dfebc99b08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNOMED and MRCONSO need to be downloaded from NIH website\n",
    "\n",
    "snomed = pd.read_csv('./Resources/der2_iisssccRefset_ExtendedMapFull_US1000124_20250901.txt', sep='\\t')\n",
    "snomed = snomed.loc[~snomed['mapTarget'].isna()].sort_values('referencedComponentId')\n",
    "snomed = snomed[['referencedComponentId','mapGroup','mapPriority','mapRule','mapAdvice','mapTarget']]\n",
    "snomed = snomed.loc[snomed['mapRule'] == 'TRUE']\n",
    "snomed.to_pickle('./Resources/snomed_20250901.pkl')\n",
    "\n",
    "###\n",
    "\n",
    "pd.read_csv('./Resources/MRCONSO.RRF', sep='|', header=None)[[0,11,13,14]].to_pickle('./Resources/2025AB_MRCONSO.pkl')\n",
    "\n",
    "###\n",
    "\n",
    "# ! rm -rf ./Resources/der2_iisssccRefset_ExtendedMapFull_US1000124_20250901.txt\n",
    "# ! rm -rf ./Resources/MRCONSO.RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db0ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pd.read_csv('./Resources/phecodeX_info.csv')\n",
    "pi['phecodeX_code'] = pi['phecode'].str.split('_').str[1]\n",
    "\n",
    "ses = pd.read_csv('./Resources/hpo-phecodeX_linkswithHPOchildrenExpansion_StrongEvidenceSpecific.tsv', sep='\\t')\n",
    "ses['phecodeX_code'] = ses['phecodeX_code'].astype(str)\n",
    "mp = pd.read_csv('./Resources/ChildHPO_Precision_X.tsv', sep='\\t')\n",
    "mp['phecodeX_code'] = mp['phecodeX_code'].astype(str)\n",
    "\n",
    "op = pd.concat([ses[['phecodeX_code','phecodeX_label','phecodeX_category']], mp[['phecodeX_code','phecodeX_label','phecodeX_category']]])\n",
    "op = op.drop_duplicates()\n",
    "op = op.merge(pi[['phecode','phecode_string']].rename({'phecode_string':'phecodeX_label'},axis=1), how='left')\n",
    "up = op.loc[op['phecode'].isna()].drop(['phecode'],axis=1)\n",
    "op = op.loc[op['phecode'].notna()]\n",
    "up = up.merge(pi[['phecode','phecodeX_code','phecode_string']], how='inner')\n",
    "up = up.loc[~up['phecodeX_code'].isin(['962.11','324.6','352.2','374.7','375.113','765.3','767.1',\n",
    "                                       '750.12','751.2','757.2','772.1','772.2','978','978.3',\n",
    "                                       '973','975','975.2','705.12','618','582.1','582.2','582.3',\n",
    "                                       '618.1','626.4','374.4','200.11','751.4','168.3','962',\n",
    "                                       '962.1','771','754','752','753.3','977','775','770.4',\n",
    "                                       '770.3','772','771.1','771.3','976','969'])]\n",
    "up = up.drop('phecode_string',axis=1)\n",
    "op = pd.concat([op,up]).drop_duplicates()\n",
    "op = op.merge(pi[['phecode','phecode_string']])\n",
    "\n",
    "ses = ses.merge(op)[['phecode','hpo_code','phecode_string','hpo_label','hpo_child','StrongEvidenceSpecific','StrongEvidenceBroad']]\n",
    "mp = mp.merge(op)[['phecode','hpo_code','phecode_string','hpo_label','hpo_child','StrongEvidenceSpecific','StrongEvidenceBroad']]\n",
    "mp = mp.loc[(mp['StrongEvidenceBroad'] == True) | (mp['StrongEvidenceSpecific'] == True)]\n",
    "map = pd.concat([ses,mp]).drop_duplicates(['phecode','hpo_code']).rename({'hpo_code':'id'},axis=1)\n",
    "map.to_pickle('./Resources/hpo_phecodex_map.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e44b24-21db-4cb1-8192-4cc08a66645c",
   "metadata": {},
   "source": [
    "## Cleaning raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2369b69b-af79-4b42-aba9-4ff74e23cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_drugbank(refs):\n",
    "    if not isinstance(refs, (list, tuple)):\n",
    "        return None\n",
    "    for d in refs:\n",
    "        if isinstance(d, dict) and str(d.get('source', '')).lower() == 'drugbank':\n",
    "            ids = d.get('ids') or d.get('id') or d.get('identifier') or d.get('value')\n",
    "            return ids[0] if isinstance(ids, (list, tuple)) else ids\n",
    "    return None\n",
    "\n",
    "drugs = pd.read_pickle(\"./OT_Raw/molecule.pkl\")\n",
    "drugs[\"drugbank\"] = drugs[\"crossReferences\"].apply(extract_drugbank)\n",
    "\n",
    "drugs = drugs[[\n",
    "    \"id\",\"blackBoxWarning\",\"name\",\"drugbank\",\n",
    "    \"maximumClinicalTrialPhase\",\"isApproved\",\n",
    "    \"synonyms\",\"crossReferences\",\"description\"\n",
    "]]\n",
    "drugs[\"name\"] = drugs[\"name\"].str.lower()\n",
    "drugs.to_pickle(\"./OT_Final/molecule_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f438e71f-f626-48e1-9a22-211168cd0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_csv('./Resources/phecodeX_unrolled_ICD_CM.csv')\n",
    "p2 = pd.read_csv('./Resources/phecodeX_unrolled_ICD_WHO.csv')\n",
    "comb = pd.concat([p1,p2])\n",
    "comb = comb.rename({'ICD':'code'},axis=1)\n",
    "comb.loc[comb['vocabulary_id'].str.contains('ICD9'), 'icd_type'] = 'ICD9'\n",
    "comb.loc[comb['vocabulary_id'].str.contains('ICD10'), 'icd_type'] = 'ICD10'\n",
    "comb['code'] = comb['code'].astype(str)\n",
    "comb_null = comb.copy()\n",
    "comb_null['code'] = comb_null['code'].str.replace('.','', regex=False)\n",
    "\n",
    "hp = pd.read_pickle('./Resources/hpo_phecodex_map.pkl')[['id','phecode']].drop_duplicates()\n",
    "hp['code'] = hp['id'].str.split('_').str[1].astype(str)\n",
    "hp = hp[['phecode','code']]\n",
    "hp['terminology'] = 'HP'\n",
    "\n",
    "umls = pd.read_pickle('./Resources/2025AB_MRCONSO.pkl')[[0,11,13]]\n",
    "umls_icd = umls.loc[umls[11].str.contains('ICD9|ICD10')]\n",
    "umls_icd = umls_icd.merge(comb, left_on=13, right_on='code')[[0,'phecode']].drop_duplicates()\n",
    "umls_other = umls.loc[~umls[11].str.contains('ICD9|ICD10')]\n",
    "umls_other = umls_other.merge(umls_icd, on=0)\n",
    "umls_other = umls_other[[13,11,'phecode']].rename({13:'code',11:'umls_terminology'},axis=1).drop_duplicates()\n",
    "umls_icd = umls_icd.set_axis(['code','phecode'],axis=1)\n",
    "umls_icd['umls_terminology'] = 'UMLS'\n",
    "umls = pd.concat([umls_icd,umls_other])\n",
    "\n",
    "snomed = pd.read_pickle('./Resources/snomed_20250901.pkl')\n",
    "snomed = snomed.merge(comb[['phecode','code']], left_on='mapTarget', right_on='code')\n",
    "snomed = snomed[['referencedComponentId','phecode']].drop_duplicates().set_axis(['code','phecode'],axis=1)\n",
    "snomed['code'] = snomed['code'].astype(str)\n",
    "snomed['terminology'] = 'SNOMEDCT'\n",
    "\n",
    "disease = pd.read_pickle(\"./OT_Raw/diseases.pkl\")[['id','dbXRefs','name']]\n",
    "disease = disease.explode('dbXRefs').dropna(subset='dbXRefs')\n",
    "disease['terminology'] = disease['dbXRefs'].str.split(':').str[0]\n",
    "disease['code'] = disease['dbXRefs'].str.split(':').str[1].astype(str)\n",
    "disease = disease[['id','name','terminology','code']]\n",
    "disease.loc[disease['terminology'].str.contains('ICD9'), 'icd_type'] = 'ICD9'\n",
    "disease.loc[disease['terminology'].str.contains('ICD10'), 'icd_type'] = 'ICD10'\n",
    "\n",
    "disease1 = disease.merge(comb, on=['code','icd_type'], how='left')\n",
    "disease2 = disease.merge(hp, on=['code','terminology'], how='left')\n",
    "disease3 = disease.merge(snomed, on=['code','terminology'], how='left')\n",
    "\n",
    "disease_umls = disease.copy()\n",
    "disease_umls = disease_umls.merge(umls)\n",
    "check = pd.read_excel('./Resources/umls_equivalence.xlsx').dropna()\n",
    "disease_umls = disease_umls.merge(check)\n",
    "disease_umls = disease_umls[['id','name','terminology','code','phecode']]\n",
    "\n",
    "temp = disease1.loc[(disease1['phecode'].isna()) & (disease1['icd_type'].notna())].drop(['phecode','vocabulary_id'],axis=1)\n",
    "temp['code'] = temp['code'].str.replace('*','').str.replace('+','')\n",
    "temp = temp.merge(comb, on=['code','icd_type'], how='left')\n",
    "disease1 = pd.concat([disease1, temp])\n",
    "\n",
    "temp = temp.loc[(temp['phecode'].isna()) & (temp['icd_type'].notna())].drop(['phecode','vocabulary_id'],axis=1)\n",
    "temp = temp.loc[~temp['code'].str.contains('-')]\n",
    "temp['code'] = temp['code'].str.replace('.','')\n",
    "temp = temp.merge(comb_null, on=['code','icd_type'], how='left')\n",
    "disease1 = pd.concat([disease1, temp])\n",
    "\n",
    "temp = temp.loc[(temp['phecode'].isna()) & (temp['icd_type'].notna())].drop(['phecode','vocabulary_id'],axis=1)\n",
    "temp['code'] = temp['code'].str[:-1]\n",
    "temp = temp.merge(comb_null, on=['code','icd_type'], how='left')\n",
    "disease1 = pd.concat([disease1, temp])\n",
    "\n",
    "disease = pd.concat([disease1,disease2,disease3,disease_umls]).drop(['icd_type','vocabulary_id'],axis=1).drop_duplicates()\n",
    "disease.to_pickle('./OT_Final/diseases_cleaned.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f89e29-f679-41ac-9c07-88942f480fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indications = pd.read_pickle(\"./OT_Raw/indication.pkl\")\n",
    "\n",
    "exploded_indications = indications[['id','approvedIndications']].explode('approvedIndications').dropna()\n",
    "exploded_indications = exploded_indications.rename({'approvedIndications':'disease'},axis=1)\n",
    "exploded_indications['maxPhaseForIndication'] = 4\n",
    "\n",
    "# Function to process each row and extract the desired information\n",
    "def process_row(row):\n",
    "    id_value = row['id']\n",
    "    result_rows = []\n",
    "    for indication in row['indications']:\n",
    "        disease = indication['disease']\n",
    "        max_phase = indication['maxPhaseForIndication']\n",
    "        result_rows.append({'id': id_value, 'disease': disease, 'maxPhaseForIndication': max_phase})\n",
    "    return result_rows\n",
    "\n",
    "# Apply the function to each row and create a list of new rows\n",
    "new_rows = []\n",
    "for _, row in indications.iterrows():\n",
    "    new_rows.extend(process_row(row))\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "expanded_df = pd.DataFrame(new_rows)\n",
    "\n",
    "indications = pd.concat([expanded_df, exploded_indications])\n",
    "indications = indications.rename({'maxPhaseForIndication':'phase'},axis=1)\n",
    "indications = indications.sort_values(['id','disease','phase'], ascending=[True,True,False])\n",
    "indications = indications.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "map = pd.read_pickle('./OT_Final/diseases_cleaned.pkl')\n",
    "map['code'] = map['code'].astype(str)\n",
    "\n",
    "phecode_map = map.loc[map['phecode'].notna()][['id','phecode']]\n",
    "\n",
    "mondo_map = map.loc[map['terminology'] == 'MONDO'][['code','id']]\n",
    "mondo_map = mondo_map.merge(phecode_map)\n",
    "mondo_map['id'] = 'MONDO_' + mondo_map['code'].astype(str)\n",
    "mondo_map = mondo_map[['id','phecode']]\n",
    "\n",
    "hp_map = map.loc[map['terminology'] == 'HP'][['code','id']]\n",
    "hp_map = hp_map.merge(phecode_map)\n",
    "hp_map['id'] = 'HP_' + hp_map['code'].astype(str)\n",
    "hp_map = hp_map[['id','phecode']]\n",
    "\n",
    "orpha_map = map.loc[map['terminology'] == 'Orphanet'][['code','id']]\n",
    "orpha_map = orpha_map.merge(phecode_map)\n",
    "orpha_map['id'] = 'Orphanet_' + orpha_map['code'].astype(str)\n",
    "orpha_map = orpha_map[['id','phecode']]\n",
    "\n",
    "efo_map = map.loc[map['terminology'] == 'EFO'][['code','id']]\n",
    "efo_map = efo_map.merge(phecode_map)\n",
    "efo_map['id'] = 'EFO_' + efo_map['code'].astype(str)\n",
    "efo_map = efo_map[['id','phecode']]\n",
    "\n",
    "hp = pd.read_pickle('./Resources/hpo_phecodex_map.pkl')[['id','phecode']].drop_duplicates()\n",
    "\n",
    "eva_map = pd.concat([hp, phecode_map, mondo_map, hp_map, orpha_map, efo_map]).drop_duplicates(['id','phecode'])\n",
    "eva_map = eva_map.rename({'id':'disease'},axis=1)\n",
    "\n",
    "indications = indications.merge(eva_map)\n",
    "indications = indications.groupby(['id','disease','phecode'])['phase'].max().reset_index()\n",
    "indications.to_pickle('./OT_Final/indication_cleaned.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d3e158-3e06-499b-b0b1-0ffc16ab4b82",
   "metadata": {},
   "source": [
    "## HGMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d82894-8959-426f-b81a-0651f60e24ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_712538/3741145667.py:19: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./Resources/2023_4_hg38_fullinfo.tsv', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# Mapping HGMD\n",
    "\n",
    "hid = pd.read_csv('./Resources/HGMD_HPO_Parent_and_Root_2023_4_Mort.csv')\n",
    "hid = hid.assign(parentAndRoot_HPO_codes=hid['parentAndRoot_HPO_codes'].str.split(',')).explode('parentAndRoot_HPO_codes')\n",
    "hid['code'] = hid['parentAndRoot_HPO_codes'].str.replace('HP:','').str.lstrip('0')\n",
    "hid = hid[['acc_num','code']]\n",
    "hid['code'] = hid['code'].astype(str)\n",
    "\n",
    "#####\n",
    "\n",
    "hp = pd.read_pickle('./Resources/hpo_phecodex_map.pkl')[['phecode','id']]\n",
    "hp['code'] = hp['id'].str.replace('HP_', '').str.lstrip('0').astype(int).astype(str)\n",
    "hp = hp[['code','phecode']].drop_duplicates()\n",
    "\n",
    "hid = hid.merge(hp).drop_duplicates().rename({'acc_num':'ID'},axis=1)\n",
    "\n",
    "#####\n",
    "\n",
    "df = pd.read_csv('./Resources/2023_4_hg38_fullinfo.tsv', sep='\\t')\n",
    "def parse_info(info_string):\n",
    "    info_dict = {}\n",
    "    for item in info_string.split(';'):\n",
    "        if '=' in item:\n",
    "            key, value = item.split('=', 1)\n",
    "            info_dict[key] = value.strip('\"')\n",
    "    return info_dict\n",
    "parsed_info = df['INFO'].apply(parse_info)\n",
    "df_parsed = pd.DataFrame(parsed_info.tolist())\n",
    "\n",
    "df = df.join(df_parsed)[['CHROM','POS','REF','ALT','DNA','ID','CLASS','GENE']]\n",
    "\n",
    "#####\n",
    "\n",
    "hgmd = df.merge(hid)\n",
    "hgmd.to_pickle('./OT_Final/hgmd.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1e558-eda4-4ceb-bc79-8ba74ef16e5b",
   "metadata": {},
   "source": [
    "## OMIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44a7cde-8a85-4931-a50f-b3038b10183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_712538/3792239263.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df.loc[df['Phenotypes'].str.contains('(3)')]\n"
     ]
    }
   ],
   "source": [
    "p1 = pd.read_csv('./Resources/phecodeX_unrolled_ICD_CM.csv')\n",
    "p2 = pd.read_csv('./Resources/phecodeX_unrolled_ICD_WHO.csv')\n",
    "comb = pd.concat([p1,p2])\n",
    "comb = comb.rename({'ICD':'code'},axis=1)\n",
    "comb.loc[comb['vocabulary_id'].str.contains('ICD9'), 'icd_type'] = 'ICD9'\n",
    "comb.loc[comb['vocabulary_id'].str.contains('ICD10'), 'icd_type'] = 'ICD10'\n",
    "comb = comb[['phecode','code']]\n",
    "\n",
    "umls = pd.read_pickle('./Resources/2025AB_MRCONSO.pkl')[[0,11,13]]\n",
    "\n",
    "umls_icd = umls.loc[umls[11].str.contains('ICD9|ICD10')]\n",
    "umls_icd = umls_icd.merge(comb, left_on=13, right_on='code')[[0,'phecode']].drop_duplicates()\n",
    "\n",
    "umls_omim = umls.loc[umls[11] == 'OMIM']\n",
    "umls_omim = umls_omim.loc[~umls_omim[13].str.contains('MTHU')]\n",
    "umls_omim = umls_omim.loc[~umls_omim[13].str.contains('.', regex=False)]\n",
    "umls_omim[13] = umls_omim[13].astype(int)\n",
    "umls_omim = umls_omim[[0,13]].merge(umls_icd, on=0)\n",
    "umls_omim = umls_omim[[13,'phecode']].set_axis(['code','phecode'],axis=1)\n",
    "\n",
    "map = pd.read_pickle('./OT_Final/diseases_cleaned.pkl')\n",
    "map = map.loc[map['terminology'].str.contains('OMIM')]\n",
    "map = map.loc[map['phecode'].notna()][['code','phecode']]\n",
    "map['code'] = map['code'].astype(int)\n",
    "map = pd.concat([map,umls_omim]).drop_duplicates()\n",
    "\n",
    "df = pd.read_csv('./Resources/genemap2_20251114.txt', sep='\\t', skiprows=3)[['Approved Gene Symbol','Phenotypes']].dropna()\n",
    "df['Phenotypes'] = df['Phenotypes'].str.split(';')\n",
    "df = df.explode('Phenotypes')\n",
    "df = df.loc[df['Phenotypes'].str.contains('(3)')]\n",
    "\n",
    "df['MIM'] = df['Phenotypes'].str.extract(r',\\s*(\\d+)\\s*\\(')\n",
    "df['MIM'] = pd.to_numeric(df['MIM'], errors='coerce')\n",
    "df = df[['Approved Gene Symbol','MIM']].set_axis(['Gene','MIM'],axis=1).dropna()\n",
    "df['MIM'] = df['MIM'].astype(int)\n",
    "df = df.rename({'MIM':'code'},axis=1)\n",
    "df = df.merge(map)\n",
    "df = df[['Gene','phecode']].drop_duplicates()\n",
    "df.to_pickle('./OT_Final/omim.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713cd7aa-79b7-4bc8-9596-9d8ced780322",
   "metadata": {},
   "source": [
    "## OT Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6679e4-2f7f-45d8-92d8-96ae0b7c42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = pd.read_pickle('./OT_Final/diseases_cleaned.pkl')\n",
    "map['code'] = map['code'].astype(str)\n",
    "\n",
    "phecode_map = map.loc[map['phecode'].notna()][['id','phecode']]\n",
    "\n",
    "mondo_map = map.loc[map['terminology'] == 'MONDO'][['code','id']]\n",
    "mondo_map = mondo_map.merge(phecode_map)\n",
    "mondo_map['id'] = 'MONDO_' + mondo_map['code'].astype(str)\n",
    "mondo_map = mondo_map[['id','phecode']]\n",
    "\n",
    "hp_map = map.loc[map['terminology'] == 'HP'][['code','id']]\n",
    "hp_map = hp_map.merge(phecode_map)\n",
    "hp_map['id'] = 'HP_' + hp_map['code'].astype(str)\n",
    "hp_map = hp_map[['id','phecode']]\n",
    "\n",
    "orpha_map = map.loc[map['terminology'] == 'Orphanet'][['code','id']]\n",
    "orpha_map = orpha_map.merge(phecode_map)\n",
    "orpha_map['id'] = 'Orphanet_' + orpha_map['code'].astype(str)\n",
    "orpha_map = orpha_map[['id','phecode']]\n",
    "\n",
    "efo_map = map.loc[map['terminology'] == 'EFO'][['code','id']]\n",
    "efo_map = efo_map.merge(phecode_map)\n",
    "efo_map['id'] = 'EFO_' + efo_map['code'].astype(str)\n",
    "efo_map = efo_map[['id','phecode']]\n",
    "\n",
    "hp = pd.read_pickle('./Resources/hpo_phecodex_map.pkl')[['phecode','id']]\n",
    "\n",
    "eva_map = pd.concat([phecode_map, mondo_map, hp_map, orpha_map, efo_map, hp]).drop_duplicates(['id','phecode'])\n",
    "eva_map = eva_map.rename({'id':'diseaseId'},axis=1)\n",
    "\n",
    "#####\n",
    "\n",
    "target = pd.read_pickle('./OT_Raw/targets.pkl')[['id','approvedSymbol']]\n",
    "target = target.set_axis(['targetId','gene'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5905a534-96eb-40eb-8f26-81b56bcf5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = pd.read_pickle('./OT_Raw/direct.pkl')\n",
    "dbs = dbs.merge(target)\n",
    "dbs = dbs.merge(eva_map)\n",
    "dbs = dbs.sort_values('score',ascending=False).drop_duplicates(['phecode','gene','datasourceId','score','evidenceCount'])\n",
    "dbs = pd.pivot_table(dbs, index=['phecode','gene'], columns='datasourceId', values='score').reset_index()\n",
    "dbs.to_pickle('./OT_Final/direct_by_source.pkl')\n",
    "\n",
    "# Not used\n",
    "if False:\n",
    "    dbs = pd.read_pickle('./OT_Raw/indirect.pkl')\n",
    "    dbs = dbs.merge(target)\n",
    "    dbs = dbs.merge(eva_map)\n",
    "    dbs = dbs.sort_values('score',ascending=False).drop_duplicates(['phecode','gene','datasourceId','score','evidenceCount'])\n",
    "    dbs = pd.pivot_table(dbs, index=['phecode','gene'], columns='datasourceId', values='score').reset_index()\n",
    "    dbs.to_pickle('./OT_Final/indirect_by_source.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4487f7bf-9d1c-45da-bc2b-af97d1c85614",
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl = pd.read_pickle('./OT_Raw/chembl.pkl')\n",
    "chembl = chembl.merge(target).merge(eva_map)[['datasourceId','phecode','gene','clinicalPhase',\n",
    "                                              'drugId','studyStartDate','studyStopReasonCategories','score','directionOnTrait']]\n",
    "chembl = chembl.rename({'clinicalPhase':'phase','drugId':'chembl_id'},axis=1)\n",
    "chembl.to_pickle('./OT_Final/chembl.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a22dc9-da8a-4d97-8b21-68beab8bfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl = pd.read_pickle('./OT_Raw/chembl.pkl')\n",
    "chembl = chembl.merge(target).merge(eva_map)\n",
    "chembl['id'] = chembl['phecode'] + ':' + chembl['gene']\n",
    "chembl['active'] = chembl['clinicalStatus'].map({'Completed':0,'Terminated':0,'Recruiting':1,'Unknown status':0,\n",
    "                             'Active, not recruiting':1,'Withdrawn':0,'Not yet recruiting':1,\n",
    "                             'Suspended':0,'Enrolling by invitation':1,'None':0})\n",
    "\n",
    "chembl = chembl[['drugId','id','clinicalPhase','clinicalStatus','studyId','active','studyStartDate','studyStopReasonCategories']]\n",
    "chembl = chembl.set_axis(['chembl_id','id','phase','status','study_id','active','start_date','stop_reason'],axis=1)\n",
    "chembl['start_date'] = pd.to_datetime(chembl['start_date'])\n",
    "chembl.loc[chembl['status'].astype(str).str.contains('Completed'),'Negative'] = 2\n",
    "chembl.loc[chembl['stop_reason'].astype(str).str.contains('Negative'),'Negative'] = 1\n",
    "chembl.loc[chembl['status'].astype(str).str.contains('Completed'),'Safety'] = 2\n",
    "chembl.loc[chembl['stop_reason'].astype(str).str.contains('Safety'),'Safety'] = 1\n",
    "chembl['active'] = chembl['active'].fillna(0)\n",
    "\n",
    "mk = pd.read_pickle('./Resources/minikel_drugs_cleaned.pkl')\n",
    "mk['id'] = mk['phecode'] + ':' + mk['gene']\n",
    "mk['phase'] = mk['phase'].map({'Preclinical':0.5,'Phase I':1,'Phase II':2,'Phase III':3,'Launched':4})\n",
    "mk['active'] = 0\n",
    "mk.loc[mk['active_max_phase'].notna(), 'active'] = 1\n",
    "mk = mk[['id','phase','active','succ_p_1','succ_1_2','succ_2_3','succ_3_a']]\n",
    "mk = mk.set_axis(['id','max_phase','active','p_1','1_2','2_3','3_a'],axis=1)\n",
    "\n",
    "#####\n",
    "\n",
    "temp = chembl[['chembl_id','id','phase','study_id','status','active','start_date']]\n",
    "temp = pd.concat([temp, mk[['id','max_phase','active']].rename({'max_phase':'phase'},axis=1)])\n",
    "temp = temp.reset_index(drop=True).drop_duplicates()\n",
    "temp.to_pickle('./Resources/all_trials.pkl')\n",
    "\n",
    "#####\n",
    "\n",
    "ed = chembl.groupby('id')['start_date'].min().reset_index()\n",
    "ed.to_pickle('./Resources/earliest_date.pkl')\n",
    "\n",
    "#####\n",
    "\n",
    "ps = chembl.groupby(['id'])[['phase','active']].max().reset_index()\n",
    "ps = ps.rename({'phase':'max_phase'},axis=1)\n",
    "ps = ps.merge(chembl.groupby(['id','phase'])['start_date'].min().reset_index().rename({'phase':'max_phase'},axis=1), how='left')\n",
    "\n",
    "ps.loc[ps['max_phase'] >= 1, 'p_1'] = 1\n",
    "ps.loc[(ps['max_phase'] == 0.5) & (ps['active'] == 0), 'p_1'] = 0\n",
    "\n",
    "ps.loc[ps['max_phase'] >= 2, '1_2'] = 1\n",
    "ps.loc[(ps['max_phase'] == 1) & (ps['active'] == 0), '1_2'] = 0\n",
    "\n",
    "ps.loc[ps['max_phase'] >= 3, '2_3'] = 1\n",
    "ps.loc[(ps['max_phase'] == 2) & (ps['active'] == 0), '2_3'] = 0\n",
    "\n",
    "ps.loc[ps['max_phase'] >= 4, '3_a'] = 1\n",
    "ps.loc[(ps['max_phase'] == 3) & (ps['active'] == 0), '3_a'] = 0\n",
    "\n",
    "ps.loc[ps['max_phase'] >= 4, '1_a'] = 1\n",
    "ps.loc[(ps['max_phase'].isin([1,2,3])) & (ps['active'] == 0), '1_a'] = 0\n",
    "\n",
    "ps = pd.concat([ps,mk]).groupby('id')[['max_phase','p_1','1_2','2_3','3_a','1_a']].max().reset_index()\n",
    "ps.to_pickle('./Resources/phase_success.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f73265-87c8-4175-8440-0efab304cb09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
